Описание проекта
Делаем в командах по 4-5 человек (делитесь самостоятельно, можно и не в рамках
одного ВУЗа). Защита будет 10.12.2025. На защиту отводится 10 минут примерно (7
минут доклад, 3 минуты вопросы). Пожалуйста, укладывайтесь в тайминг. Таблица для
записи на защиты будет позднее (защиты будут с 11.10 до 16.00 мск).
Дедлайн загрузки работы: 08.12.2025, чтобы мы успели посмотреть (презентацию
тоже сдайте, но её еще можно будет до защиты немного доделывать). Форму также
ближе к сдаче опубликуем.
Что нужно сделать:
1. (2) Выбрать источник данных (сайт), который вы будете парсить. Это может
быть книжный магазин, сайт с цитатами и т.д. Прошу вас, берите реальные
сайте (не надо брать какой-нибудь сайт, который специально сделан для
скрейпинга). Здесь можно использовать Beautiful Soup. Но будет плюсом, если
посмотрите Selenium/Playwright.
2. (2) Собрать данные с сайта (не менее 1500 строк и не менее 6 колонок данных
разных). Сохранить в pandas и потом в csv файл. Вы наверняка столкнетесь с
трудностями типа капчи и т.д. Добавить пользовательские метрики (например,
длина отзыва).
3. (1) Сделать базовый анализ с помощью pandas. Обработать дубликаты,
пропуски, аномальные значения. Посчитать не менее 15 разных характеристик
данных, по которым можно будет сделать выводы и закономерности. Вы
можете какие-то характеристики дополнительно сформировать на основе
данных.
4. (1) Сделать базовый анализ с помощью matplotlib. Сделать не мене 10
различных графиков, которые характеризуют данные. Графики должны быть
красивыми, с подписями и т.д. В каждом графике добавьте короткий вывод под
ним.
5. (2) Подключить API GigaChat. С помощью которого проанализировать данные
(например, отзывы, тональность отзывов, тематику отзывов). Из вашей базы
выберите 200 строк, для каждой из которых вы сделаете запросы с API
GigaChat. Вот тут примеры работы (токен только так не храните). Важно, что он
на основе ваших данных должен будет дать еще дополнительные
характеристики.
Что сдаем:
1. Файл с кодом (лучше colab) + всеми выводами и визуализациями
2. Презентацию, в которой описываем:
1) Титул + команда
• Фотки
• Описание ролей (кто парсил / кто делал анализ / кто строил график / кто
с API работал)
2) Описание и цель
Сайт надо описать, что делали, что нашли
3) Источник данных + robot.txt. Что нашли в нем?
Скрин robot.txt обязательно!
4) Метод сбора данных + трудности
• капча
• динамическая подгрузка
• блокировки
5) Структура данных
Покажите таблицы, которые получились.
6) Pandas-анализ
Топ-5 интересных выводов, примеры, ключевые закономерности,
характеристики и выводы, какие были аномалии и пропуски.
7) Графики
6–8 графиков, интересные выводы.
8) GigaChat
• примеры промптов
• новая таблица признаков (эмоции, тональность и т.д.)
• С какими сложностями при работе с API столкнулись?
9) Инсайты/Выводы
Оценка:
1. Защита — блокер, она обязательна. Если не прийти, будет 0.
2. Разбалловка за каждый пункт приведена в задании. Обратите внимание, что
сейчас там 8 баллов. За что можно получить 10 баллов? Например, за
использование вещей, которые мы не прошли — Selenium/Playwright (в т.ч.
комбинаций с Beautiful Soup), за работу с другими API AI, которыми мы не
работаем, и на которых не давалось примеров. Или же, работа с
динамическими страничками тоже будет плюсом. Можно и дашборд какой-то
прикрутить. То есть дополнительные задание, использование дополнительных
вещей пройденных самостоятельно оценивается на 9-10 баллов, при условии
выполнения остальног